### SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes

Novel view synthesis for dynamic scenes is still a challenging problem in computer vision and graphics. Recently, Gaussian splatting has emerged as a robust technique to represent static scenes and enable high-quality and real-time novel view synthesis. Building upon this technique, we propose a new representation that explicitly decomposes the motion and appearance of dynamic scenes into sparse control points and dense Gaussians, respectively. Our key idea is to use sparse control points, significantly fewer in number than the Gaussians, to learn compact 6 DoF transformation bases, which can be locally interpolated through learned interpolation weights to yield the motion field of 3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF transformations for each control point, which reduces learning complexities, enhances learning abilities, and facilitates obtaining temporal and spatial coherent motion patterns. Then, we jointly learn the 3D Gaussians, the canonical space locations of control points, and the deformation MLP to reconstruct the appearance, geometry, and dynamics of 3D scenes. During learning, the location and number of control points are adaptively adjusted to accommodate varying motion complexities in different regions, and an ARAP loss following the principle of as rigid as possible is developed to enforce spatial continuity and local rigidity of learned motions. Finally, thanks to the explicit sparse motion representation and its decomposition from appearance, our method can enable user-controlled motion editing while retaining high-fidelity appearances. Extensive experiments demonstrate that our approach outperforms existing approaches on novel view synthesis with a high rendering speed and enables novel appearance-preserved motion editing applications.

在计算机视觉和图形学中，动态场景的新视角合成仍然是一个挑战性问题。最近，高斯涂抹技术作为一种表示静态场景的稳健技术浮现出来，使得高质量和实时的新视角合成成为可能。基于这种技术，我们提出了一种新的表征方式，它显式地将动态场景的运动和外观分解为稀疏控制点和密集高斯模型。我们的关键思想是使用数量远少于高斯模型的稀疏控制点来学习紧凑的6自由度（DoF）转换基础，这些基础可以通过学习到的插值权重在局部插值，从而产生3D高斯模型的运动场。我们采用变形多层感知器（MLP）来预测每个控制点的时变6自由度转换，这降低了学习复杂性，增强了学习能力，并有助于获得时空连贯的运动模式。然后，我们联合学习3D高斯模型、控制点的典型空间位置和变形MLP，以重建3D场景的外观、几何和动态特性。在学习过程中，控制点的位置和数量会根据不同区域的运动复杂性自适应地调整，并且开发了一个遵循尽可能刚性原则的ARAP（As Rigid As Possible）损失，以强制学习到的运动具有空间连续性和局部刚性。最后，由于明确的稀疏运动表征及其与外观的分解，我们的方法能够在保持高保真外观的同时，实现用户控制的运动编辑。广泛的实验表明，我们的方法在新视角合成方面超越了现有方法，并具有高渲染速度，使得新的保持外观的运动编辑应用成为可能。

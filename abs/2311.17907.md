### CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting

With the onset of diffusion-based generative models and their ability to generate text-conditioned images, content generation has received a massive invigoration. Recently, these models have been shown to provide useful guidance for the generation of 3D graphics assets. However, existing work in text-conditioned 3D generation faces fundamental constraints: (i) inability to generate detailed, multi-object scenes, (ii) inability to textually control multi-object configurations, and (iii) physically realistic scene composition. In this work, we propose CG3D, a method for compositionally generating scalable 3D assets that resolves these constraints. We find that explicit Gaussian radiance fields, parameterized to allow for compositions of objects, possess the capability to enable semantically and physically consistent scenes. By utilizing a guidance framework built around this explicit representation, we show state of the art results, capable of even exceeding the guiding diffusion model in terms of object combinations and physics accuracy.

随着基于扩散的生成模型的出现以及它们生成文本条件图像的能力，内容生成得到了巨大的振兴。最近，这些模型已被证明可为3D图形资产的生成提供有用的指导。然而，现有的文本条件3D生成工作面临着基本限制：（i）无法生成详细的多对象场景，（ii）无法通过文本控制多对象配置，以及（iii）物理上逼真的场景构成。在这项工作中，我们提出了CG3D，一种用于组合生成可扩展3D资产的方法，解决了这些约束。我们发现，明确的高斯辐射场，被参数化以允许对象的组合，具备实现语义和物理上一致场景的能力。通过利用围绕这种明确表示构建的指导框架，我们展示了最新的成果，甚至能够在对象组合和物理精确度方面超越引导扩散模型。

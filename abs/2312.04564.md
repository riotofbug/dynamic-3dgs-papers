### EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS

Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach results in scene representations with fewer Gaussians and quantized representations, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce memory by more than an order of magnitude all while maintaining the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x less memory and faster training/inference speed.

近来，三维高斯分散（3D-GS）在新视角场景合成中获得了人气。它解决了与神经辐射场（NeRFs）相关的漫长训练时间和缓慢的渲染速度的挑战。通过快速、可微的三维高斯光栅化，3D-GS实现了实时渲染和加速训练。然而，它们需要大量的内存资源用于训练和存储，因为每个场景的点云表示需要数百万个高斯点。我们提出了一种使用量化嵌入的技术，显著降低了内存存储需求，并采用了从粗到细的训练策略，以更快、更稳定地优化高斯点云。我们的方法导致使用更少的高斯点和量化表示的场景表示，从而实现了更快的训练时间和渲染速度，用于实时渲染高分辨率场景。我们在保持重建质量的同时，将内存需求减少了一个数量级以上。我们在各种数据集和场景上验证了我们方法的有效性，同时保持了视觉质量，同时消耗了比以往少10-20倍的内存，并且训练/推断速度更快。

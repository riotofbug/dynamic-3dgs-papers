### 3D Gaussian Splatting for Real-Time Radiance Field Rendering

Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.

辐射场方法最近彻底改变了使用多张照片或视频捕捉的场景的新视角合成技术。然而，要实现高视觉质量仍然需要成本高昂的神经网络来训练和渲染，而近期较快的方法不可避免地在速度和质量之间做出妥协。对于未受限且完整的场景（而不是孤立对象）和1080p分辨率渲染，目前还没有方法能实现实时显示率。我们引入了三个关键元素，使我们能够在保持竞争力的训练时间的同时，实现行业领先的视觉质量，并且重要的是，支持高质量的实时（>= 30 fps）新视角合成，分辨率为1080p。首先，从相机校准过程中产生的稀疏点出发，我们用3D高斯分布来表示场景，它保留了连续体积辐射场对场景优化的有益属性，同时避免了在空旷空间中的不必要计算；其次，我们进行3D高斯的交错优化/密度控制，特别是优化各向异性协方差以实现场景的精确表现；第三，我们开发了一种快速的可感知可见性的渲染算法，支持各向异性溅射，加速了训练并允许实时渲染。我们在几个已建立的数据集上展示了行业领先的视觉质量和实时渲染。

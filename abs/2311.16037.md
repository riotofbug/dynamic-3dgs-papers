### GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions

Recently, impressive results have been achieved in 3D scene editing with text instructions based on a 2D diffusion model. However, current diffusion models primarily generate images by predicting noise in the latent space, and the editing is usually applied to the whole image, which makes it challenging to perform delicate, especially localized, editing for 3D scenes. Inspired by recent 3D Gaussian splatting, we propose a systematic framework, named GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text instructions. Benefiting from the explicit property of 3D Gaussians, we design a series of techniques to achieve delicate editing. Specifically, we first extract the region of interest (RoI) corresponding to the text instruction, aligning it to 3D Gaussians. The Gaussian RoI is further used to control the editing process. Our framework can achieve more delicate and precise editing of 3D scenes than previous methods while enjoying much faster training speed, i.e. within 20 minutes on a single V100 GPU, more than twice as fast as Instruct-NeRF2NeRF (45 minutes -- 2 hours).

近期，在基于2D扩散模型的3D场景文本指令编辑方面取得了令人印象深刻的成果。然而，目前的扩散模型主要通过预测潜空间中的噪声来生成图像，而编辑通常应用于整个图像，这使得对3D场景进行精细的、尤其是局部的编辑变得具有挑战性。受到最近3D高斯飞溅的启发，我们提出了一个名为GaussianEditor的系统框架，通过3D高斯和文本指令来精细地编辑3D场景。得益于3D高斯的显式属性，我们设计了一系列技术来实现精细编辑。具体来说，我们首先提取与文本指令相对应的兴趣区域（RoI），并将其与3D高斯对齐。接着，利用高斯RoI来控制编辑过程。我们的框架能够比以前的方法更精细、更准确地编辑3D场景，同时享受更快的训练速度，即在单个V100 GPU上仅需20分钟，比Instruct-NeRF2NeRF（45分钟至2小时）快两倍以上。

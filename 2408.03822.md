### Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields

3D Gaussian splatting (3DGS) has recently emerged as an alternative representation that leverages a 3D Gaussian-based representation and introduces an approximated volumetric rendering, achieving very fast rendering speed and promising image quality. Furthermore, subsequent studies have successfully extended 3DGS to dynamic 3D scenes, demonstrating its wide range of applications. However, a significant drawback arises as 3DGS and its following methods entail a substantial number of Gaussians to maintain the high fidelity of the rendered images, which requires a large amount of memory and storage. To address this critical issue, we place a specific emphasis on two key objectives: reducing the number of Gaussian points without sacrificing performance and compressing the Gaussian attributes, such as view-dependent color and covariance. To this end, we propose a learnable mask strategy that significantly reduces the number of Gaussians while preserving high performance. In addition, we propose a compact but effective representation of view-dependent color by employing a grid-based neural field rather than relying on spherical harmonics. Finally, we learn codebooks to compactly represent the geometric and temporal attributes by residual vector quantization. With model compression techniques such as quantization and entropy coding, we consistently show over 25x reduced storage and enhanced rendering speed compared to 3DGS for static scenes, while maintaining the quality of the scene representation. For dynamic scenes, our approach achieves more than 12x storage efficiency and retains a high-quality reconstruction compared to the existing state-of-the-art methods. Our work provides a comprehensive framework for 3D scene representation, achieving high performance, fast training, compactness, and real-time rendering. Our project page is available at this https URL.

3D 高斯泼溅 (3DGS) 最近成为一种替代表示，它利用基于 3D 高斯的表示并引入近似体积渲染，实现非常快的渲染速度和良好的图像质量此外，后续研究已成功将 3DGS 扩展到动态 3D 场景。 ，展示了其广泛的应用，但是，由于 3DGS 及其后续方法需要大量的高斯来维持渲染图像的高保真度，因此需要大量的内存和存储来解决这个关键问题。问题中，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，例如依赖于视图的颜色和协方差。为此，我们提出了一种可学习的掩模策略，可以显着减少高斯点的数量。此外，我们通过采用基于网格的神经场而不是依赖于球谐函数，提出了一种紧凑但有效的视图相关颜色表示。最后，我们学习密码本来紧凑地表示几何和时间。通过量化和熵编码等模型压缩技术，我们的方法始终显示，与 3DGS 相比，静态场景的存储量减少了 25 倍，渲染速度提高了，同时保持了动态场景的场景表示质量。与现有最先进的方法相比，实现了超过 12 倍的存储效率并保留了高质量的重建。我们的工作为 3D 场景表示提供了全面的框架，实现了高性能、快速训练、紧凑性和实时性。我们的项目页面可通过此 https URL 获取。
